{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3YLyiTwPfVCT"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/magenta/ddsp/blob/main/ddsp/colab/demos/timbre_transfer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "\n",
        "##### Copyright 2021 Google LLC.\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bvp6GWqtfVCW"
      },
      "outputs": [],
      "source": [
        "# Copyright 2021 Google LLC. All Rights Reserved.\n",
        "\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# =============================================================================="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JndnmDMp66FL"
      },
      "source": [
        "# DDSP Timbre Transfer Demo\n",
        "\n",
        "This notebook is a demo of timbre transfer using DDSP (Differentiable Digital Signal Processing). \n",
        "The model here is trained to generate audio conditioned on a time series of fundamental frequency and loudness. \n",
        "\n",
        "* [DDSP ICLR paper](https://openreview.net/forum?id=B1x1ma4tDr)\n",
        "* [Audio Examples](http://goo.gl/magenta/ddsp-examples) \n",
        "\n",
        "This notebook extracts these features from input audio (either uploaded files, or recorded from the microphone) and resynthesizes with the model. \n",
        "\n",
        "<img src=\"https://magenta.tensorflow.org/assets/ddsp/ddsp_cat_jamming.png\" alt=\"DDSP Tone Transfer\" width=\"700\">\n",
        "\n",
        "\n",
        "\n",
        "By default, the notebook will download pre-trained models. You can train a model on your own sounds by using the [Train Autoencoder Colab](https://github.com/magenta/ddsp/blob/main/ddsp/colab/demos/train_autoencoder.ipynb).\n",
        "\n",
        "Have fun! And please feel free to hack this notebook to make your own creative interactions.\n",
        "\n",
        "\n",
        "### Instructions for running:\n",
        "\n",
        "* Make sure to use a GPU runtime, click:  __Runtime >> Change Runtime Type >> GPU__\n",
        "* Press ▶️ on the left of each of the cells\n",
        "* View the code: Double-click any of the cells\n",
        "* Hide the code: Double click the right side of the cell\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "6wZde6CBya9k"
      },
      "outputs": [],
      "source": [
        "#@title # Step 1: Install DDSP\n",
        "\n",
        "#@markdown Install ddsp in a conda environment with Python 3.9 for compatibility.\n",
        "#@markdown This transfers a lot of data and _should take about 5 minutes_.\n",
        "#@markdown You can ignore warnings.\n",
        "\n",
        "!rm -rf /content/miniconda\n",
        "!curl -L https://repo.anaconda.com/miniconda/Miniconda3-py39_23.11.0-2-Linux-x86_64.sh -o miniconda.sh\n",
        "!chmod +x miniconda.sh\n",
        "!sh miniconda.sh -b -p /content/miniconda\n",
        "!sudo apt-get install -y libportaudio2\n",
        "!/content/miniconda/bin/conda install -y -c conda-forge cudatoolkit=11.2 cudnn=8.1\n",
        "!/content/miniconda/bin/pip install tensorflow==2.11 tensorflow-probability==0.19.0 tensorflowjs==3.18.0 tensorflow-datasets==4.9.0 tflite-support==0.1.0a1 ddsp==3.7.0 hmmlearn\n",
        "print('\\nDone installing DDSP in conda environment!')\n",
        "\n",
        "\n",
        "SCRIPT = r'''\n",
        "\"\"\"DDSP Timbre Transfer inference script.\n",
        "Runs inside conda environment with Python 3.9 and ddsp==3.7.0.\n",
        "Reads input audio and parameters, writes output audio and plot data.\n",
        "\"\"\"\n",
        "import argparse\n",
        "import copy\n",
        "import os\n",
        "import pickle\n",
        "import time\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import ddsp\n",
        "import ddsp.training\n",
        "from ddsp.training.postprocessing import detect_notes, fit_quantile_transform\n",
        "import gin\n",
        "import tensorflow.compat.v2 as tf\n",
        "\n",
        "\n",
        "def get_tuning_factor(f0_midi, f0_confidence, mask_on):\n",
        "  \"\"\"Get offset in cents to most consistent chromatic intervals.\"\"\"\n",
        "  tuning_factors = np.linspace(-0.5, 0.5, 101)\n",
        "  midi_diffs = (f0_midi[mask_on][:, np.newaxis] -\n",
        "                tuning_factors[np.newaxis, :]) % 1.0\n",
        "  midi_diffs[midi_diffs > 0.5] -= 1.0\n",
        "  weights = f0_confidence[mask_on][:, np.newaxis]\n",
        "  cost_diffs = np.abs(midi_diffs)\n",
        "  cost_diffs = np.mean(weights * cost_diffs, axis=0)\n",
        "  f0_at = f0_midi[mask_on][:, np.newaxis] - midi_diffs\n",
        "  f0_at_diffs = np.diff(f0_at, axis=0)\n",
        "  deltas = (f0_at_diffs != 0.0).astype(float)\n",
        "  cost_deltas = np.mean(weights[:-1] * deltas, axis=0)\n",
        "  norm = lambda x: (x - np.mean(x)) / np.std(x)\n",
        "  cost = norm(cost_deltas) + norm(cost_diffs)\n",
        "  return tuning_factors[np.argmin(cost)]\n",
        "\n",
        "\n",
        "def auto_tune(f0_midi, tuning_factor, mask_on, amount=0.0):\n",
        "  \"\"\"Reduce variance of f0 from scale intervals.\"\"\"\n",
        "  major_scale = np.ravel(\n",
        "      [np.array([0, 2, 4, 5, 7, 9, 11]) + 12 * i for i in range(10)])\n",
        "  all_scales = np.stack([major_scale + i for i in range(12)])\n",
        "  f0_on = f0_midi[mask_on]\n",
        "  f0_diff_tsn = (\n",
        "      f0_on[:, np.newaxis, np.newaxis] - all_scales[np.newaxis, :, :])\n",
        "  f0_diff_ts = np.min(np.abs(f0_diff_tsn), axis=-1)\n",
        "  f0_diff_s = np.mean(f0_diff_ts, axis=0)\n",
        "  scale_idx = np.argmin(f0_diff_s)\n",
        "  f0_diff_tn = f0_midi[:, np.newaxis] - all_scales[scale_idx][np.newaxis, :]\n",
        "  note_idx = np.argmin(np.abs(f0_diff_tn), axis=-1)\n",
        "  midi_diff = np.take_along_axis(\n",
        "      f0_diff_tn, note_idx[:, np.newaxis], axis=-1)[:, 0]\n",
        "  return f0_midi - amount * midi_diff\n",
        "\n",
        "\n",
        "def shift_ld(audio_features, ld_shift=0.0):\n",
        "  audio_features['loudness_db'] += ld_shift\n",
        "  return audio_features\n",
        "\n",
        "\n",
        "def shift_f0(audio_features, pitch_shift=0.0):\n",
        "  audio_features['f0_hz'] *= 2.0 ** (pitch_shift)\n",
        "  audio_features['f0_hz'] = np.clip(audio_features['f0_hz'], 0.0,\n",
        "                                    librosa.midi_to_hz(110.0))\n",
        "  return audio_features\n",
        "\n",
        "\n",
        "def find_model_dir(dir_name):\n",
        "  for root, dirs, filenames in os.walk(dir_name):\n",
        "    for filename in filenames:\n",
        "      if filename.endswith(\".gin\") and not filename.startswith(\".\"):\n",
        "        return root\n",
        "  return dir_name\n",
        "\n",
        "\n",
        "def main():\n",
        "  import librosa  # imported here so shift_f0 has access\n",
        "  globals()['librosa'] = librosa\n",
        "\n",
        "  parser = argparse.ArgumentParser()\n",
        "  parser.add_argument('--audio_path', required=True)\n",
        "  parser.add_argument('--output_dir', default='/content/output')\n",
        "  parser.add_argument('--model', default='Violin')\n",
        "  parser.add_argument('--threshold', type=float, default=1.0)\n",
        "  parser.add_argument('--adjust', type=int, default=1)\n",
        "  parser.add_argument('--quiet', type=float, default=20.0)\n",
        "  parser.add_argument('--autotune', type=float, default=0.0)\n",
        "  parser.add_argument('--pitch_shift', type=float, default=0.0)\n",
        "  parser.add_argument('--loudness_shift', type=float, default=0.0)\n",
        "  args = parser.parse_args()\n",
        "\n",
        "  os.makedirs(args.output_dir, exist_ok=True)\n",
        "\n",
        "  # Load audio\n",
        "  audio = np.load(args.audio_path)\n",
        "  print(f'Loaded audio: shape={audio.shape}')\n",
        "\n",
        "  # --- Load Model ---\n",
        "  PRETRAINED_MODELS = ['Violin', 'Flute', 'Flute2', 'Trumpet', 'Tenor_Saxophone']\n",
        "\n",
        "  if args.model in PRETRAINED_MODELS:\n",
        "    PRETRAINED_DIR = '/content/pretrained'\n",
        "    os.system(f'rm -rf {PRETRAINED_DIR}')  \n",
        "    os.makedirs(PRETRAINED_DIR, exist_ok=True)\n",
        "    GCS_CKPT_DIR = 'gs://ddsp/models/timbre_transfer_colab/2021-07-08'\n",
        "    model_dir_gcs = os.path.join(GCS_CKPT_DIR, 'solo_%s_ckpt' % args.model.lower())\n",
        "    os.system(f'gsutil cp {model_dir_gcs}/* {PRETRAINED_DIR}')  \n",
        "    model_dir = PRETRAINED_DIR\n",
        "  else:\n",
        "    # Custom model: assume uploaded and unzipped at /content/uploaded\n",
        "    model_dir = find_model_dir('/content/uploaded')\n",
        "\n",
        "  gin_file = os.path.join(model_dir, 'operative_config-0.gin')\n",
        "  print(f'Using model dir: {model_dir}')\n",
        "\n",
        "  # Load dataset statistics\n",
        "  DATASET_STATS = None\n",
        "  dataset_stats_file = os.path.join(model_dir, 'dataset_statistics.pkl')\n",
        "  print(f'Loading dataset statistics from {dataset_stats_file}')\n",
        "  try:\n",
        "    if tf.io.gfile.exists(dataset_stats_file):\n",
        "      with tf.io.gfile.GFile(dataset_stats_file, 'rb') as f:\n",
        "        DATASET_STATS = pickle.load(f)\n",
        "  except Exception as err:\n",
        "    print('Loading dataset statistics failed: {}'.format(err))\n",
        "\n",
        "  # Parse gin config\n",
        "  with gin.unlock_config():\n",
        "    gin.parse_config_file(gin_file, skip_unknown=True)\n",
        "\n",
        "  # --- Compute Audio Features ---\n",
        "  print('Computing audio features...')\n",
        "  ddsp.spectral_ops.reset_crepe()\n",
        "  start_time = time.time()\n",
        "  audio_features = ddsp.training.metrics.compute_audio_features(audio)\n",
        "  audio_features = {k: v.numpy() if hasattr(v, 'numpy') else v for k, v in audio_features.items()}\n",
        "  audio_features['loudness_db'] = audio_features['loudness_db'].astype(np.float32)\n",
        "  print('Audio features took %.1f seconds' % (time.time() - start_time))\n",
        "\n",
        "  # Ensure dimensions match\n",
        "  ckpt_files = [f for f in tf.io.gfile.listdir(model_dir) if 'ckpt' in f]\n",
        "  ckpt_name = ckpt_files[0].split('.')[0]\n",
        "  ckpt = os.path.join(model_dir, ckpt_name)\n",
        "\n",
        "  time_steps_train = gin.query_parameter('F0LoudnessPreprocessor.time_steps')\n",
        "  n_samples_train = gin.query_parameter('Harmonic.n_samples')\n",
        "  hop_size = int(n_samples_train / time_steps_train)\n",
        "\n",
        "  time_steps = int(audio.shape[1] / hop_size)\n",
        "  n_samples = time_steps * hop_size\n",
        "\n",
        "  gin_params = [\n",
        "      'Harmonic.n_samples = {}'.format(n_samples),\n",
        "      'FilteredNoise.n_samples = {}'.format(n_samples),\n",
        "      'F0LoudnessPreprocessor.time_steps = {}'.format(time_steps),\n",
        "      'oscillator_bank.use_angular_cumsum = True',\n",
        "  ]\n",
        "  with gin.unlock_config():\n",
        "    gin.parse_config(gin_params)\n",
        "\n",
        "  # Trim features\n",
        "  for key in ['f0_hz', 'f0_confidence', 'loudness_db']:\n",
        "    audio_features[key] = audio_features[key][:time_steps]\n",
        "  audio_features['audio'] = audio_features['audio'][:, :n_samples]\n",
        "\n",
        "  # --- Modify Conditioning ---\n",
        "  audio_features_mod = {k: v.copy() for k, v in audio_features.items()}\n",
        "  mask_on = None\n",
        "\n",
        "  if args.adjust and DATASET_STATS is not None:\n",
        "    mask_on, note_on_value = detect_notes(\n",
        "        audio_features['loudness_db'],\n",
        "        audio_features['f0_confidence'],\n",
        "        args.threshold)\n",
        "\n",
        "    if np.any(mask_on):\n",
        "      # Shift pitch register\n",
        "      target_mean_pitch = DATASET_STATS['mean_pitch']\n",
        "      pitch = ddsp.core.hz_to_midi(audio_features['f0_hz'])\n",
        "      mean_pitch = np.mean(pitch[mask_on])\n",
        "      p_diff = target_mean_pitch - mean_pitch\n",
        "      p_diff_octave = p_diff / 12.0\n",
        "      round_fn = np.floor if p_diff_octave > 1.5 else np.ceil\n",
        "      p_diff_octave = round_fn(p_diff_octave)\n",
        "      audio_features_mod = shift_f0(audio_features_mod, p_diff_octave)\n",
        "\n",
        "      # Quantile shift\n",
        "      _, loudness_norm = fit_quantile_transform(\n",
        "          audio_features['loudness_db'],\n",
        "          mask_on,\n",
        "          inv_quantile=DATASET_STATS['quantile_transform'])\n",
        "\n",
        "      mask_off = np.logical_not(mask_on)\n",
        "      loudness_norm[mask_off] -= args.quiet * (\n",
        "          1.0 - note_on_value[mask_off][:, np.newaxis])\n",
        "      loudness_norm = np.reshape(loudness_norm,\n",
        "                                 audio_features['loudness_db'].shape)\n",
        "      audio_features_mod['loudness_db'] = loudness_norm\n",
        "\n",
        "      # Auto-tune\n",
        "      if args.autotune:\n",
        "        f0_midi = np.array(ddsp.core.hz_to_midi(audio_features_mod['f0_hz']))\n",
        "        tuning_factor = get_tuning_factor(\n",
        "            f0_midi, audio_features_mod['f0_confidence'], mask_on)\n",
        "        f0_midi_at = auto_tune(f0_midi, tuning_factor, mask_on,\n",
        "                               amount=args.autotune)\n",
        "        audio_features_mod['f0_hz'] = ddsp.core.midi_to_hz(f0_midi_at)\n",
        "    else:\n",
        "      print('Skipping auto-adjust (no notes detected).')\n",
        "  else:\n",
        "    print('Skipping auto-adjust (disabled or no dataset statistics).')\n",
        "\n",
        "  # Manual shifts\n",
        "  audio_features_mod = shift_ld(audio_features_mod, args.loudness_shift)\n",
        "  audio_features_mod = shift_f0(audio_features_mod, args.pitch_shift)\n",
        "\n",
        "  # --- Build and Run Model ---\n",
        "  print('Loading model and running inference...')\n",
        "  model = ddsp.training.models.Autoencoder()\n",
        "  model.restore(ckpt)\n",
        "\n",
        "  af = audio_features_mod\n",
        "\n",
        "  start_time = time.time()\n",
        "  _ = model(af, training=False)\n",
        "  print('Model build took %.1f seconds' % (time.time() - start_time))\n",
        "\n",
        "  start_time = time.time()\n",
        "  outputs = model(af, training=False)\n",
        "  audio_gen = model.get_audio_from_outputs(outputs)\n",
        "  print('Prediction took %.1f seconds' % (time.time() - start_time))\n",
        "\n",
        "  # --- Save Outputs ---\n",
        "  np.save(os.path.join(args.output_dir, 'audio_gen.npy'), np.array(audio_gen))\n",
        "  np.save(os.path.join(args.output_dir, 'audio_orig.npy'), np.array(audio_features['audio']))\n",
        "\n",
        "  # Save plot data\n",
        "  plot_data = {\n",
        "      'loudness_db_orig': audio_features['loudness_db'],\n",
        "      'loudness_db_mod': audio_features_mod['loudness_db'],\n",
        "      'f0_hz_orig': audio_features['f0_hz'],\n",
        "      'f0_hz_mod': audio_features_mod['f0_hz'],\n",
        "      'f0_confidence': audio_features['f0_confidence'],\n",
        "      'mask_on': mask_on,\n",
        "  }\n",
        "  if mask_on is not None and np.any(mask_on):\n",
        "    plot_data['note_on_value'] = note_on_value\n",
        "\n",
        "  with open(os.path.join(args.output_dir, 'plot_data.pkl'), 'wb') as f:\n",
        "    pickle.dump(plot_data, f)\n",
        "\n",
        "  print('Done! Outputs saved to', args.output_dir)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  main()\n",
        "'''\n",
        "\n",
        "with open('/content/timbre_transfer_inference.py', 'w') as f:\n",
        "  f.write(SCRIPT)\n",
        "print('Inference script written to /content/timbre_transfer_inference.py')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Go36QW9AS_CD"
      },
      "outputs": [],
      "source": [
        "#@title # Step 2: Record or Upload Audio\n",
        "#@markdown * Either record audio from microphone or upload audio from file (.mp3 or .wav) \n",
        "#@markdown * Audio should be monophonic (single instrument / voice)\n",
        "\n",
        "record_or_upload = \"Record\"  #@param [\"Record\", \"Upload (.mp3 or .wav)\"]\n",
        "\n",
        "record_seconds =     5#@param {type:\"number\", min:1, max:10, step:1}\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import base64\n",
        "import io\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display\n",
        "from scipy.io import wavfile\n",
        "\n",
        "from google.colab import files as colab_files\n",
        "from google.colab import output\n",
        "\n",
        "SAMPLE_RATE = 16000\n",
        "\n",
        "\n",
        "def play(array_of_floats, sample_rate=SAMPLE_RATE):\n",
        "  \"\"\"Play audio in colab using HTML5 audio widget.\"\"\"\n",
        "  if len(array_of_floats.shape) == 2:\n",
        "    array_of_floats = array_of_floats[0]\n",
        "  normalizer = float(np.iinfo(np.int16).max)\n",
        "  array_of_ints = np.array(\n",
        "      np.asarray(array_of_floats) * normalizer, dtype=np.int16)\n",
        "  memfile = io.BytesIO()\n",
        "  wavfile.write(memfile, sample_rate, array_of_ints)\n",
        "  html = \"\"\"<audio controls>\n",
        "              <source controls src=\"data:audio/wav;base64,{base64_wavfile}\"\n",
        "              type=\"audio/wav\" />\n",
        "              Your browser does not support the audio element.\n",
        "            </audio>\"\"\"\n",
        "  html = html.format(\n",
        "      base64_wavfile=base64.b64encode(memfile.getvalue()).decode('ascii'))\n",
        "  memfile.close()\n",
        "  display.display(display.HTML(html))\n",
        "\n",
        "\n",
        "def record_audio(seconds=3, sample_rate=SAMPLE_RATE):\n",
        "  \"\"\"Record audio from the browser microphone.\"\"\"\n",
        "  record_js_code = \"\"\"\n",
        "  const sleep  = time => new Promise(resolve => setTimeout(resolve, time))\n",
        "  const b2text = blob => new Promise(resolve => {\n",
        "    const reader = new FileReader()\n",
        "    reader.onloadend = e => resolve(e.srcElement.result)\n",
        "    reader.readAsDataURL(blob)\n",
        "  })\n",
        "\n",
        "  var record = time => new Promise(async resolve => {\n",
        "    stream = await navigator.mediaDevices.getUserMedia({ audio: true })\n",
        "    recorder = new MediaRecorder(stream)\n",
        "    chunks = []\n",
        "    recorder.ondataavailable = e => chunks.push(e.data)\n",
        "    recorder.start()\n",
        "    await sleep(time)\n",
        "    recorder.onstop = async ()=>{\n",
        "      blob = new Blob(chunks)\n",
        "      text = await b2text(blob)\n",
        "      resolve(text)\n",
        "    }\n",
        "    recorder.stop()\n",
        "  })\n",
        "  \"\"\"\n",
        "  print('Starting recording for {} seconds...'.format(seconds))\n",
        "  display.display(display.Javascript(record_js_code))\n",
        "  audio_string = output.eval_js('record(%d)' % (seconds * 1000.0))\n",
        "  print('Finished recording!')\n",
        "  audio_bytes = base64.b64decode(audio_string.split(',')[1])\n",
        "  # Convert bytes to numpy using pydub\n",
        "  from pydub import AudioSegment\n",
        "  segment = AudioSegment.from_file(io.BytesIO(audio_bytes))\n",
        "  segment = segment.set_frame_rate(sample_rate).set_channels(1).set_sample_width(2)\n",
        "  samples = np.array(segment.get_array_of_samples()).astype(np.float32)\n",
        "  samples = samples / float(np.iinfo(np.int16).max)\n",
        "  return samples\n",
        "\n",
        "\n",
        "def upload_audio(sample_rate=SAMPLE_RATE):\n",
        "  \"\"\"Upload audio files and return (filenames, audio_arrays).\"\"\"\n",
        "  from pydub import AudioSegment\n",
        "  audio_files = colab_files.upload()\n",
        "  fnames = list(audio_files.keys())\n",
        "  audios = []\n",
        "  for fname in fnames:\n",
        "    segment = AudioSegment.from_file(io.BytesIO(audio_files[fname]))\n",
        "    segment = segment.set_frame_rate(sample_rate).set_channels(1).set_sample_width(2)\n",
        "    samples = np.array(segment.get_array_of_samples()).astype(np.float32)\n",
        "    samples = samples / float(np.iinfo(np.int16).max)\n",
        "    audios.append(samples)\n",
        "  return fnames, audios\n",
        "\n",
        "\n",
        "def specplot(audio, vmin=-5, vmax=1, rotate=True, size=512 + 256):\n",
        "  \"\"\"Plot the log magnitude spectrogram of audio.\"\"\"\n",
        "  if len(audio.shape) == 2:\n",
        "    audio = audio[0]\n",
        "  # Compute spectrogram using numpy/scipy (no ddsp needed)\n",
        "  from scipy import signal as scipy_signal\n",
        "  f, t, Sxx = scipy_signal.stft(audio, fs=SAMPLE_RATE, nperseg=size,\n",
        "                                 noverlap=size * 3 // 4)\n",
        "  logmag = np.log10(np.abs(Sxx) + 1e-7)\n",
        "  if rotate:\n",
        "    logmag = np.flipud(logmag)\n",
        "  plt.matshow(logmag, vmin=vmin, vmax=vmax, cmap=plt.cm.magma, aspect='auto')\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])\n",
        "  plt.xlabel('Time')\n",
        "  plt.ylabel('Frequency')\n",
        "\n",
        "\n",
        "# --- Record or Upload ---\n",
        "if record_or_upload == \"Record\":\n",
        "  audio = record_audio(seconds=record_seconds)\n",
        "else:\n",
        "  filenames, audios = upload_audio()\n",
        "  audio = audios[0]\n",
        "\n",
        "if len(audio.shape) == 1:\n",
        "  audio = audio[np.newaxis, :]\n",
        "\n",
        "# Save audio for the inference script\n",
        "np.save('/content/input_audio.npy', audio)\n",
        "print(f'Audio shape: {audio.shape}, saved to /content/input_audio.npy')\n",
        "\n",
        "# Plot and play\n",
        "specplot(audio)\n",
        "play(audio)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "wmSGDWM5yyjm"
      },
      "outputs": [],
      "source": [
        "#@title # Step 3: Write Inference Script\n",
        "#@markdown This cell writes the DDSP inference script that will run inside the conda environment.\n",
        "#@markdown **You do not need to modify this cell.**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "uQFUlIJ_5r36"
      },
      "outputs": [],
      "source": [
        "#@title # Step 4: Run Timbre Transfer\n",
        "\n",
        "#@markdown Choose a model and adjust parameters, then run this cell.\n",
        "#@markdown To try different settings, change the values and re-run this cell.\n",
        "\n",
        "model = 'Violin' #@param ['Violin', 'Flute', 'Flute2', 'Trumpet', 'Tenor_Saxophone', 'Upload your own (checkpoint folder as .zip)']\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown ## Note Detection\n",
        "#@markdown You can leave this at 1.0 for most cases\n",
        "threshold = 1 #@param {type:\"slider\", min: 0.0, max:2.0, step:0.01}\n",
        "\n",
        "#@markdown ## Automatic Adjustments\n",
        "ADJUST = True #@param{type:\"boolean\"}\n",
        "\n",
        "#@markdown Quiet parts without notes detected (dB)\n",
        "quiet = 20 #@param {type:\"slider\", min: 0, max:60, step:1}\n",
        "\n",
        "#@markdown Force pitch to nearest note (amount)\n",
        "autotune = 0 #@param {type:\"slider\", min: 0.0, max:1.0, step:0.1}\n",
        "\n",
        "#@markdown ## Manual Adjustments\n",
        "#@markdown Shift the pitch (octaves)\n",
        "pitch_shift =  0 #@param {type:\"slider\", min:-2, max:2, step:1}\n",
        "\n",
        "#@markdown Adjust the overall loudness (dB)\n",
        "loudness_shift = 0 #@param {type:\"slider\", min:-20, max:20, step:1}\n",
        "\n",
        "# Handle custom model upload\n",
        "if model == 'Upload your own (checkpoint folder as .zip)':\n",
        "  UPLOAD_DIR = '/content/uploaded'\n",
        "  !mkdir -p $UPLOAD_DIR\n",
        "  from google.colab import files\n",
        "  uploaded_files = files.upload()\n",
        "  for fnames in uploaded_files.keys():\n",
        "    print(\"Unzipping... {}\".format(fnames))\n",
        "    !unzip -o \"/content/$fnames\" -d $UPLOAD_DIR &> /dev/null\n",
        "  model_arg = 'custom'\n",
        "else:\n",
        "  model_arg = model\n",
        "\n",
        "adjust_flag = 1 if ADJUST else 0\n",
        "\n",
        "# Run inference in conda environment\n",
        "cmd = (\n",
        "    \"unset PYTHONPATH PYTHONHOME && \"\n",
        "    \"export LD_LIBRARY_PATH=/content/miniconda/lib:$LD_LIBRARY_PATH && \"\n",
        "    \"/content/miniconda/bin/python /content/timbre_transfer_inference.py \"\n",
        "    f\"--audio_path=/content/input_audio.npy \"\n",
        "    f\"--output_dir=/content/output \"\n",
        "    f\"--model={model_arg} \"\n",
        "    f\"--threshold={threshold} \"\n",
        "    f\"--adjust={adjust_flag} \"\n",
        "    f\"--quiet={quiet} \"\n",
        "    f\"--autotune={autotune} \"\n",
        "    f\"--pitch_shift={pitch_shift} \"\n",
        "    f\"--loudness_shift={loudness_shift}\"\n",
        ")\n",
        "print('Running timbre transfer...')\n",
        "!{cmd}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "SLwg1WkHCXQO"
      },
      "outputs": [],
      "source": [
        "#@title # Step 5: Listen to Results\n",
        "\n",
        "#@markdown Load and display the timbre transfer results.\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import base64\n",
        "import io\n",
        "import pickle\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display\n",
        "from scipy.io import wavfile\n",
        "\n",
        "SAMPLE_RATE = 16000\n",
        "\n",
        "\n",
        "def play(array_of_floats, sample_rate=SAMPLE_RATE):\n",
        "  if len(array_of_floats.shape) == 2:\n",
        "    array_of_floats = array_of_floats[0]\n",
        "  normalizer = float(np.iinfo(np.int16).max)\n",
        "  array_of_ints = np.array(\n",
        "      np.asarray(array_of_floats) * normalizer, dtype=np.int16)\n",
        "  memfile = io.BytesIO()\n",
        "  wavfile.write(memfile, sample_rate, array_of_ints)\n",
        "  html = \"\"\"<audio controls>\n",
        "              <source controls src=\"data:audio/wav;base64,{base64_wavfile}\"\n",
        "              type=\"audio/wav\" />\n",
        "              Your browser does not support the audio element.\n",
        "            </audio>\"\"\"\n",
        "  html = html.format(\n",
        "      base64_wavfile=base64.b64encode(memfile.getvalue()).decode('ascii'))\n",
        "  memfile.close()\n",
        "  display.display(display.HTML(html))\n",
        "\n",
        "\n",
        "def specplot(audio, vmin=-5, vmax=1, rotate=True, size=512 + 256):\n",
        "  if len(audio.shape) == 2:\n",
        "    audio = audio[0]\n",
        "  from scipy import signal as scipy_signal\n",
        "  f, t, Sxx = scipy_signal.stft(audio, fs=SAMPLE_RATE, nperseg=size,\n",
        "                                 noverlap=size * 3 // 4)\n",
        "  logmag = np.log10(np.abs(Sxx) + 1e-7)\n",
        "  if rotate:\n",
        "    logmag = np.flipud(logmag)\n",
        "  plt.matshow(logmag, vmin=vmin, vmax=vmax, cmap=plt.cm.magma, aspect='auto')\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])\n",
        "  plt.xlabel('Time')\n",
        "  plt.ylabel('Frequency')\n",
        "\n",
        "\n",
        "# Load outputs\n",
        "audio_gen = np.load('/content/output/audio_gen.npy')\n",
        "audio_orig = np.load('/content/output/audio_orig.npy')\n",
        "\n",
        "with open('/content/output/plot_data.pkl', 'rb') as f:\n",
        "  plot_data = pickle.load(f)\n",
        "\n",
        "# --- Feature Plots ---\n",
        "import librosa\n",
        "\n",
        "TRIM = -15\n",
        "mask_on = plot_data.get('mask_on')\n",
        "has_mask = mask_on is not None and np.any(mask_on)\n",
        "n_plots = 3 if has_mask else 2\n",
        "\n",
        "fig, axes = plt.subplots(nrows=n_plots, ncols=1, sharex=True,\n",
        "                         figsize=(2*n_plots, 8))\n",
        "\n",
        "if has_mask:\n",
        "  note_on_value = plot_data['note_on_value']\n",
        "  ax = axes[0]\n",
        "  ax.plot(np.ones_like(mask_on[:TRIM]) * 1.0, 'k:')\n",
        "  ax.plot(note_on_value[:TRIM])\n",
        "  ax.plot(mask_on[:TRIM])\n",
        "  ax.set_ylabel('Note-on Mask')\n",
        "  ax.set_xlabel('Time step [frame]')\n",
        "  ax.legend(['Threshold', 'Likelihood', 'Mask'])\n",
        "\n",
        "offset = 1 if has_mask else 0\n",
        "\n",
        "ax = axes[0 + offset]\n",
        "ax.plot(plot_data['loudness_db_orig'][:TRIM])\n",
        "ax.plot(plot_data['loudness_db_mod'][:TRIM])\n",
        "ax.set_ylabel('loudness_db')\n",
        "ax.legend(['Original', 'Adjusted'])\n",
        "\n",
        "ax = axes[1 + offset]\n",
        "try:\n",
        "  ax.plot(librosa.hz_to_midi(plot_data['f0_hz_orig'][:TRIM]))\n",
        "  ax.plot(librosa.hz_to_midi(plot_data['f0_hz_mod'][:TRIM]))\n",
        "except:\n",
        "  ax.plot(plot_data['f0_hz_orig'][:TRIM])\n",
        "  ax.plot(plot_data['f0_hz_mod'][:TRIM])\n",
        "ax.set_ylabel('f0 [midi]')\n",
        "_ = ax.legend(['Original', 'Adjusted'])\n",
        "plt.show()\n",
        "\n",
        "# --- Audio Playback ---\n",
        "print('Original')\n",
        "play(audio_orig)\n",
        "\n",
        "print('Resynthesis')\n",
        "play(audio_gen)\n",
        "\n",
        "# --- Spectrograms ---\n",
        "specplot(audio_orig)\n",
        "plt.title('Original')\n",
        "plt.show()\n",
        "\n",
        "specplot(audio_gen)\n",
        "_ = plt.title('Resynthesis')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "3YLyiTwPfVCT"
      ],
      "last_runtime": {},
      "name": "timbre_transfer.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
