{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3YLyiTwPfVCT"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/magenta/ddsp/blob/main/ddsp/colab/demos/pitch_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "\n",
        "##### Copyright 2021 Google LLC.\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bvp6GWqtfVCW"
      },
      "outputs": [],
      "source": [
        "# Copyright 2021 Google LLC. All Rights Reserved.\n",
        "\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# =============================================================================="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JndnmDMp66FL"
      },
      "source": [
        "# DDSP Pitch Detection Demo\n",
        "\n",
        "This notebook is a demo of pitch detection using inverse audio synthesis. \n",
        "\n",
        "\n",
        "* [ICML Workshop paper](https://openreview.net/forum?id=RlVTYWhsky7)\n",
        "* [Audio Examples](http://goo.gl/magenta/ddsp-inv) \n",
        "\n",
        "This notebook extracts these features from input audio (either uploaded files, or recorded from the microphone) and resynthesizes the audio from the model. The DDSP-INV model is hierarchical, and provides both resynthesis from the sinusoidal model and harmonic model.\n",
        "\n",
        "<img src=\"https://storage.googleapis.com/ddsp-inv/full_stack/diagram.png\" alt=\"DDSP Pitch Detection\" width=\"700\">\n",
        "\n",
        "\n",
        "### Instructions for running:\n",
        "\n",
        "* Make sure to use a GPU runtime, click:  __Runtime >> Change Runtime Type >> GPU__\n",
        "* Press ▶️ on the left of each of the cells\n",
        "* View the code: Double-click any of the cells\n",
        "* Hide the code: Double click the right side of the cell\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "6wZde6CBya9k"
      },
      "outputs": [],
      "source": [
        "#@title # Step 1: Install DDSP\n",
        "\n",
        "#@markdown Install ddsp in a conda environment with Python 3.9 for compatibility.\n",
        "#@markdown This transfers a lot of data and _should take about 5 minutes_.\n",
        "#@markdown You can ignore warnings.\n",
        "\n",
        "!rm -rf /content/miniconda\n",
        "!curl -L https://repo.anaconda.com/miniconda/Miniconda3-py39_23.11.0-2-Linux-x86_64.sh -o miniconda.sh\n",
        "!chmod +x miniconda.sh\n",
        "!sh miniconda.sh -b -p /content/miniconda\n",
        "!sudo apt-get install -y libportaudio2\n",
        "!/content/miniconda/bin/conda install -y -c conda-forge cudatoolkit=11.2 cudnn=8.1\n",
        "!/content/miniconda/bin/pip install tensorflow==2.11 tensorflow-probability==0.19.0 tensorflowjs==3.18.0 tensorflow-datasets==4.9.0 tflite-support==0.1.0a1 ddsp==3.7.0 hmmlearn\n",
        "print('\\nDone installing DDSP in conda environment!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Go36QW9AS_CD"
      },
      "outputs": [],
      "source": [
        "#@title # Step 2: Record or Upload Audio\n",
        "#@markdown * Either record audio from microphone or upload audio from file (.mp3 or .wav) \n",
        "#@markdown * Audio should be monophonic (single instrument / voice)\n",
        "\n",
        "record_or_upload = \"Upload (.mp3 or .wav)\"  #@param [\"Record\", \"Upload (.mp3 or .wav)\"]\n",
        "\n",
        "record_seconds =     5#@param {type:\"number\", min:1, max:10, step:1}\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import base64\n",
        "import io\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display\n",
        "from scipy.io import wavfile\n",
        "\n",
        "from google.colab import files as colab_files\n",
        "from google.colab import output\n",
        "\n",
        "SAMPLE_RATE = 16000\n",
        "\n",
        "\n",
        "def play(array_of_floats, sample_rate=SAMPLE_RATE):\n",
        "  \"\"\"Play audio in colab using HTML5 audio widget.\"\"\"\n",
        "  if len(array_of_floats.shape) == 2:\n",
        "    array_of_floats = array_of_floats[0]\n",
        "  normalizer = float(np.iinfo(np.int16).max)\n",
        "  array_of_ints = np.array(\n",
        "      np.asarray(array_of_floats) * normalizer, dtype=np.int16)\n",
        "  memfile = io.BytesIO()\n",
        "  wavfile.write(memfile, sample_rate, array_of_ints)\n",
        "  html = \"\"\"<audio controls>\n",
        "              <source controls src=\"data:audio/wav;base64,{base64_wavfile}\"\n",
        "              type=\"audio/wav\" />\n",
        "              Your browser does not support the audio element.\n",
        "            </audio>\"\"\"\n",
        "  html = html.format(\n",
        "      base64_wavfile=base64.b64encode(memfile.getvalue()).decode('ascii'))\n",
        "  memfile.close()\n",
        "  display.display(display.HTML(html))\n",
        "\n",
        "\n",
        "def record_audio(seconds=3, sample_rate=SAMPLE_RATE):\n",
        "  \"\"\"Record audio from the browser microphone.\"\"\"\n",
        "  record_js_code = \"\"\"\n",
        "  const sleep  = time => new Promise(resolve => setTimeout(resolve, time))\n",
        "  const b2text = blob => new Promise(resolve => {\n",
        "    const reader = new FileReader()\n",
        "    reader.onloadend = e => resolve(e.srcElement.result)\n",
        "    reader.readAsDataURL(blob)\n",
        "  })\n",
        "\n",
        "  var record = time => new Promise(async resolve => {\n",
        "    stream = await navigator.mediaDevices.getUserMedia({ audio: true })\n",
        "    recorder = new MediaRecorder(stream)\n",
        "    chunks = []\n",
        "    recorder.ondataavailable = e => chunks.push(e.data)\n",
        "    recorder.start()\n",
        "    await sleep(time)\n",
        "    recorder.onstop = async ()=>{\n",
        "      blob = new Blob(chunks)\n",
        "      text = await b2text(blob)\n",
        "      resolve(text)\n",
        "    }\n",
        "    recorder.stop()\n",
        "  })\n",
        "  \"\"\"\n",
        "  print('Starting recording for {} seconds...'.format(seconds))\n",
        "  display.display(display.Javascript(record_js_code))\n",
        "  audio_string = output.eval_js('record(%d)' % (seconds * 1000.0))\n",
        "  print('Finished recording!')\n",
        "  audio_bytes = base64.b64decode(audio_string.split(',')[1])\n",
        "  # Convert bytes to numpy using pydub\n",
        "  from pydub import AudioSegment\n",
        "  segment = AudioSegment.from_file(io.BytesIO(audio_bytes))\n",
        "  segment = segment.set_frame_rate(sample_rate).set_channels(1).set_sample_width(2)\n",
        "  samples = np.array(segment.get_array_of_samples()).astype(np.float32)\n",
        "  samples = samples / float(np.iinfo(np.int16).max)\n",
        "  return samples\n",
        "\n",
        "\n",
        "def upload_audio(sample_rate=SAMPLE_RATE):\n",
        "  \"\"\"Upload audio files and return (filenames, audio_arrays).\"\"\"\n",
        "  from pydub import AudioSegment\n",
        "  audio_files = colab_files.upload()\n",
        "  fnames = list(audio_files.keys())\n",
        "  audios = []\n",
        "  for fname in fnames:\n",
        "    segment = AudioSegment.from_file(io.BytesIO(audio_files[fname]))\n",
        "    segment = segment.set_frame_rate(sample_rate).set_channels(1).set_sample_width(2)\n",
        "    samples = np.array(segment.get_array_of_samples()).astype(np.float32)\n",
        "    samples = samples / float(np.iinfo(np.int16).max)\n",
        "    audios.append(samples)\n",
        "  return fnames, audios\n",
        "\n",
        "\n",
        "def specplot(audio, vmin=-5, vmax=1, rotate=True, size=512 + 256):\n",
        "  \"\"\"Plot the log magnitude spectrogram of audio.\"\"\"\n",
        "  if len(audio.shape) == 2:\n",
        "    audio = audio[0]\n",
        "  # Compute spectrogram using numpy/scipy (no ddsp needed)\n",
        "  from scipy import signal as scipy_signal\n",
        "  f, t, Sxx = scipy_signal.stft(audio, fs=SAMPLE_RATE, nperseg=size,\n",
        "                                 noverlap=size * 3 // 4)\n",
        "  logmag = np.log10(np.abs(Sxx) + 1e-7)\n",
        "  if rotate:\n",
        "    logmag = np.flipud(logmag)\n",
        "  plt.matshow(logmag, vmin=vmin, vmax=vmax, cmap=plt.cm.magma, aspect='auto')\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])\n",
        "  plt.xlabel('Time')\n",
        "  plt.ylabel('Frequency')\n",
        "\n",
        "\n",
        "# --- Record or Upload ---\n",
        "if record_or_upload == \"Record\":\n",
        "  audio = record_audio(seconds=record_seconds)\n",
        "else:\n",
        "  filenames, audios = upload_audio()\n",
        "  audio = audios[0]\n",
        "\n",
        "if len(audio.shape) == 1:\n",
        "  audio = audio[np.newaxis, :]\n",
        "\n",
        "# Save audio for the inference script\n",
        "np.save('/content/input_audio.npy', audio)\n",
        "print(f'Audio shape: {audio.shape}, saved to /content/input_audio.npy')\n",
        "\n",
        "# Plot and play\n",
        "specplot(audio)\n",
        "play(audio)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "wmSGDWM5yyjm"
      },
      "outputs": [],
      "source": [
        "#@title # Step 3: Run Pitch Detection\n",
        "\n",
        "#@markdown Choose a pretrained model and run pitch detection.\n",
        "#@markdown Models separately trained on the [URMP](http://www2.ece.rochester.edu/projects/air/projects/URMP/annotations_5P.html), [MDB-stem-synth](https://zenodo.org/record/1481172#.Xzouy5NKhTY), and [MIR1k](https://sites.google.com/site/unvoicedsoundseparation/mir-1k) datasets.\n",
        "\n",
        "model = 'urmp' #@param ['urmp', 'mdb_stem_synth', 'mir1k']\n",
        "\n",
        "\n",
        "# Create inference script\n",
        "SCRIPT = r'''\n",
        "\"\"\"DDSP-INV Pitch Detection inference script.\n",
        "Runs inside conda environment with Python 3.9 and ddsp==3.7.0.\n",
        "Reads input audio, loads model, runs DDSP-INV and CREPE pitch detection,\n",
        "and writes output data as .npy files.\n",
        "\"\"\"\n",
        "import argparse\n",
        "import os\n",
        "import time\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import ddsp\n",
        "import ddsp.training\n",
        "import gin\n",
        "import tensorflow.compat.v2 as tf\n",
        "\n",
        "\n",
        "def main():\n",
        "  parser = argparse.ArgumentParser()\n",
        "  parser.add_argument('--audio_path', required=True)\n",
        "  parser.add_argument('--output_dir', default='/content/pitch_output')\n",
        "  parser.add_argument('--model', default='urmp')\n",
        "  args = parser.parse_args()\n",
        "\n",
        "  os.makedirs(args.output_dir, exist_ok=True)\n",
        "\n",
        "  # Load audio\n",
        "  audio = np.load(args.audio_path)\n",
        "  print(f'Loaded audio: shape={audio.shape}')\n",
        "\n",
        "  # --- Download and load model checkpoint ---\n",
        "  PRETRAINED_DIR = '/content/pretrained'\n",
        "  os.system(f'rm -rf {PRETRAINED_DIR}')\n",
        "  os.makedirs(PRETRAINED_DIR, exist_ok=True)\n",
        "  GCS_CKPT_DIR = 'gs://ddsp-inv/ckpts'\n",
        "  model_dir_gcs = os.path.join(GCS_CKPT_DIR, '%s_ckpt' % args.model.lower())\n",
        "  os.system(f'gsutil cp {model_dir_gcs}/* {PRETRAINED_DIR}')\n",
        "  model_dir = PRETRAINED_DIR\n",
        "\n",
        "  # Find gin config file\n",
        "  gin_file_pattern = os.path.join(model_dir, 'operative_config*.gin')\n",
        "  gin_file = tf.io.gfile.glob(gin_file_pattern)[0]\n",
        "\n",
        "  # The old checkpoints use 'TranscribingAutoencoder' as the gin class name,\n",
        "  # but it has been renamed to 'InverseSynthesis'. Patch the gin config.\n",
        "  with open(gin_file, 'r') as f:\n",
        "    gin_text = f.read()\n",
        "  gin_text = gin_text.replace('TranscribingAutoencoder', 'InverseSynthesis')\n",
        "  patched_gin_file = os.path.join(model_dir, 'patched_config.gin')\n",
        "  with open(patched_gin_file, 'w') as f:\n",
        "    f.write(gin_text)\n",
        "\n",
        "  # Parse gin config\n",
        "  with gin.unlock_config():\n",
        "    gin.parse_config_file(patched_gin_file, skip_unknown=True)\n",
        "\n",
        "  # Find checkpoint\n",
        "  ckpt_files = [f for f in tf.io.gfile.listdir(model_dir) if 'ckpt' in f]\n",
        "  ckpt_name = ckpt_files[0].split('.')[0]\n",
        "  ckpt = os.path.join(model_dir, ckpt_name)\n",
        "\n",
        "  # Ensure dimensions and sampling rates are equal\n",
        "  time_steps_train = 125\n",
        "  n_samples_train = 64000\n",
        "  hop_size = int(n_samples_train / time_steps_train)\n",
        "\n",
        "  time_steps = int(audio.shape[1] / hop_size)\n",
        "  n_samples = time_steps * hop_size\n",
        "  audio = audio[:, :n_samples]\n",
        "\n",
        "  gin_params = [\n",
        "      'InverseSynthesis.n_samples = {}'.format(n_samples),\n",
        "      'oscillator_bank.use_angular_cumsum = True',\n",
        "  ]\n",
        "\n",
        "  with gin.unlock_config():\n",
        "    gin.parse_config(gin_params)\n",
        "\n",
        "  # --- Build and restore model ---\n",
        "  print('Loading model...')\n",
        "  model = ddsp.training.models.InverseSynthesis()\n",
        "  model.restore(ckpt)\n",
        "\n",
        "  # Build model by running a batch through it.\n",
        "  start_time = time.time()\n",
        "  _ = model({'audio': audio}, training=False)\n",
        "  print('Restoring model took %.1f seconds' % (time.time() - start_time))\n",
        "\n",
        "  # --- Predict with DDSP-INV ---\n",
        "  start_time = time.time()\n",
        "  print('\\nExtracting f0 with DDSP-INV...')\n",
        "  controls = model({'audio': audio}, training=False)\n",
        "  print('Prediction took %.1f seconds' % (time.time() - start_time))\n",
        "\n",
        "  # --- Predict with CREPE ---\n",
        "  start_time = time.time()\n",
        "  print('\\nExtracting f0 with CREPE...')\n",
        "  ddsp.spectral_ops.reset_crepe()\n",
        "  f0_crepe, f0_confidence = ddsp.spectral_ops.compute_f0(\n",
        "      audio[0],\n",
        "      sample_rate=16000,\n",
        "      frame_rate=31.25,\n",
        "      viterbi=False)\n",
        "  print('Prediction took %.1f seconds' % (time.time() - start_time))\n",
        "\n",
        "  # --- Synthesize comparison audio ---\n",
        "  synth = ddsp.synths.Wavetable(n_samples=n_samples, scale_fn=None)\n",
        "  wavetable = np.sin(np.linspace(0, 2.0 * np.pi, 2048))[np.newaxis, np.newaxis, :]\n",
        "  amps = np.ones([1, time_steps, 1]) * 0.1\n",
        "  audio_crepe = synth(amps, wavetable, f0_crepe[np.newaxis, :, np.newaxis])\n",
        "  audio_ddsp_inv = synth(controls['harm_amp'], wavetable, controls['f0_hz'])\n",
        "\n",
        "  # --- Convert all tensors to numpy and save ---\n",
        "  def to_np(x):\n",
        "    return np.array(x.numpy() if hasattr(x, 'numpy') else x)\n",
        "\n",
        "  np.save(os.path.join(args.output_dir, 'audio.npy'), to_np(audio))\n",
        "  np.save(os.path.join(args.output_dir, 'sin_audio.npy'), to_np(controls['sin_audio']))\n",
        "  np.save(os.path.join(args.output_dir, 'harm_audio.npy'), to_np(controls['harm_audio']))\n",
        "  np.save(os.path.join(args.output_dir, 'f0_hz.npy'), to_np(controls['f0_hz']))\n",
        "  np.save(os.path.join(args.output_dir, 'sin_freqs.npy'), to_np(controls['sin_freqs']))\n",
        "  np.save(os.path.join(args.output_dir, 'sin_amps.npy'), to_np(controls['sin_amps']))\n",
        "  np.save(os.path.join(args.output_dir, 'harm_freqs.npy'), to_np(controls['harm_freqs']))\n",
        "  np.save(os.path.join(args.output_dir, 'harm_amps.npy'), to_np(controls['harm_amps']))\n",
        "  np.save(os.path.join(args.output_dir, 'audio_ddsp_inv.npy'), to_np(audio_ddsp_inv))\n",
        "  np.save(os.path.join(args.output_dir, 'audio_crepe.npy'), to_np(audio_crepe))\n",
        "  np.save(os.path.join(args.output_dir, 'f0_crepe.npy'), to_np(f0_crepe))\n",
        "\n",
        "  print('Done! Outputs saved to', args.output_dir)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  main()\n",
        "'''\n",
        "\n",
        "with open('/content/pitch_detection_inference.py', 'w') as f:\n",
        "  f.write(SCRIPT)\n",
        "print('Inference script written to /content/pitch_detection_inference.py')\n",
        "\n",
        "\n",
        "# Run inference in conda environment\n",
        "cmd = (\n",
        "    \"unset PYTHONPATH PYTHONHOME && \"\n",
        "    \"export LD_LIBRARY_PATH=/content/miniconda/lib:$LD_LIBRARY_PATH && \"\n",
        "    \"/content/miniconda/bin/python /content/pitch_detection_inference.py \"\n",
        "    f\"--audio_path=/content/input_audio.npy \"\n",
        "    f\"--output_dir=/content/pitch_output \"\n",
        "    f\"--model={model}\"\n",
        ")\n",
        "print('Running pitch detection...')\n",
        "!{cmd}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "jyseo-7W95AL"
      },
      "outputs": [],
      "source": [
        "#@title # Step 4: View Results\n",
        "\n",
        "#@markdown Load and display the pitch detection results.\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import base64\n",
        "import io\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display\n",
        "from scipy.io import wavfile\n",
        "\n",
        "SAMPLE_RATE = 16000\n",
        "\n",
        "\n",
        "def play(array_of_floats, sample_rate=SAMPLE_RATE):\n",
        "  if len(array_of_floats.shape) == 2:\n",
        "    array_of_floats = array_of_floats[0]\n",
        "  normalizer = float(np.iinfo(np.int16).max)\n",
        "  array_of_ints = np.array(\n",
        "      np.asarray(array_of_floats) * normalizer, dtype=np.int16)\n",
        "  memfile = io.BytesIO()\n",
        "  wavfile.write(memfile, sample_rate, array_of_ints)\n",
        "  html = \"\"\"<audio controls>\n",
        "              <source controls src=\"data:audio/wav;base64,{base64_wavfile}\"\n",
        "              type=\"audio/wav\" />\n",
        "              Your browser does not support the audio element.\n",
        "            </audio>\"\"\"\n",
        "  html = html.format(\n",
        "      base64_wavfile=base64.b64encode(memfile.getvalue()).decode('ascii'))\n",
        "  memfile.close()\n",
        "  display.display(display.HTML(html))\n",
        "\n",
        "\n",
        "def specplot(audio, vmin=-5, vmax=1, rotate=True, size=512 + 256):\n",
        "  if len(audio.shape) == 2:\n",
        "    audio = audio[0]\n",
        "  from scipy import signal as scipy_signal\n",
        "  f, t, Sxx = scipy_signal.stft(audio, fs=SAMPLE_RATE, nperseg=size,\n",
        "                                 noverlap=size * 3 // 4)\n",
        "  logmag = np.log10(np.abs(Sxx) + 1e-7)\n",
        "  if rotate:\n",
        "    logmag = np.flipud(logmag)\n",
        "  plt.matshow(logmag, vmin=vmin, vmax=vmax, cmap=plt.cm.magma, aspect='auto')\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])\n",
        "  plt.xlabel('Time')\n",
        "  plt.ylabel('Frequency')\n",
        "\n",
        "\n",
        "def hz_to_midi(hz):\n",
        "  \"\"\"Convert Hz to MIDI note number (no ddsp needed).\"\"\"\n",
        "  return 12.0 * (np.log2(np.maximum(hz, 1e-7)) - np.log2(440.0)) + 69.0\n",
        "\n",
        "\n",
        "# --- Load outputs ---\n",
        "output_dir = '/content/pitch_output'\n",
        "audio = np.load(f'{output_dir}/audio.npy')\n",
        "sin_audio = np.load(f'{output_dir}/sin_audio.npy')\n",
        "harm_audio = np.load(f'{output_dir}/harm_audio.npy')\n",
        "f0_hz = np.load(f'{output_dir}/f0_hz.npy')\n",
        "sin_freqs = np.load(f'{output_dir}/sin_freqs.npy')\n",
        "sin_amps = np.load(f'{output_dir}/sin_amps.npy')\n",
        "harm_freqs = np.load(f'{output_dir}/harm_freqs.npy')\n",
        "harm_amps = np.load(f'{output_dir}/harm_amps.npy')\n",
        "audio_ddsp_inv = np.load(f'{output_dir}/audio_ddsp_inv.npy')\n",
        "audio_crepe = np.load(f'{output_dir}/audio_crepe.npy')\n",
        "f0_crepe = np.load(f'{output_dir}/f0_crepe.npy')\n",
        "\n",
        "k = 0\n",
        "\n",
        "# --- Plot Pitch Comparison ---\n",
        "plt.figure(figsize=(6, 4))\n",
        "f0_crepe_midi = hz_to_midi(f0_crepe)\n",
        "f0_harm_midi = hz_to_midi(f0_hz)\n",
        "plt.plot(np.ravel(f0_crepe_midi), label='crepe')\n",
        "plt.plot(np.ravel(f0_harm_midi[k]), label='ddsp-inv')\n",
        "plt.ylabel('Pitch (MIDI)')\n",
        "plt.xlabel('Time')\n",
        "plt.xticks([])\n",
        "plt.legend(loc='upper right')\n",
        "plt.show()\n",
        "\n",
        "# --- Audio Playback ---\n",
        "print('Original')\n",
        "play(audio)\n",
        "\n",
        "print('Sinusoidal Resynthesis')\n",
        "play(sin_audio[k])\n",
        "\n",
        "print('Harmonic Resynthesis')\n",
        "play(harm_audio[k])\n",
        "\n",
        "print('DDSP-INV Pitch')\n",
        "play(audio_ddsp_inv[k])\n",
        "\n",
        "print('CREPE Pitch')\n",
        "play(audio_crepe[k])\n",
        "\n",
        "# --- Spectrograms ---\n",
        "specplot(audio)\n",
        "plt.title(\"Original\")\n",
        "plt.show()\n",
        "\n",
        "specplot(sin_audio[k])\n",
        "_ = plt.title(\"Sinusoidal Resynthesis\")\n",
        "plt.show()\n",
        "\n",
        "specplot(harm_audio[k])\n",
        "_ = plt.title(\"Harmonic Resynthesis\")\n",
        "plt.show()\n",
        "\n",
        "# --- Sinusoid Scatter Plots ---\n",
        "plt.figure(figsize=(6, 6))\n",
        "t = np.arange(sin_freqs.shape[1])\n",
        "for a, f in zip(np.transpose(sin_amps[k]), np.transpose(sin_freqs[k])):\n",
        "  plt.scatter(t, f, s=a*200, linewidths=1)\n",
        "  plt.ylim(0, 8000)\n",
        "plt.title('Sinusoids (Sinusoidal)')\n",
        "plt.ylabel('Frequency (Hz)')\n",
        "plt.xlabel('Time')\n",
        "plt.xticks([])\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(6, 6))\n",
        "t = np.arange(harm_freqs.shape[1])\n",
        "for a, f in zip(np.transpose(harm_amps[k]), np.transpose(harm_freqs[k])):\n",
        "  plt.scatter(t, f, s=a*200, linewidths=1)\n",
        "  plt.ylim(0, 8000)\n",
        "plt.title('Sinusoids (Harmonic)')\n",
        "plt.ylabel('Frequency (Hz)')\n",
        "plt.xlabel('Time')\n",
        "_ = plt.xticks([])\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "3YLyiTwPfVCT"
      ],
      "last_runtime": {},
      "name": "pitch_detection.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}